{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b7776d",
   "metadata": {},
   "source": [
    "## Author: Maria Garcia \n",
    "Date created: January 2, 2024\n",
    "\n",
    "# InfoSearch-Extractor\n",
    "\n",
    "## AI-Powered Document Search and Information Extraction\n",
    "\n",
    "A system that can search a given topic within a collection of PDFs and extract relevant information from the documents. This project not only involves AI and natural language processing aspects but also provides practical utility by creating a tool for efficient document search and information extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a1dfc2",
   "metadata": {},
   "source": [
    "### Overview\n",
    "Picture this: 891 e-books, a treasure trove of Data Science and Machine Learning wonders. Why? Because book lovers are on a quest for knowledge! \n",
    "\n",
    "### The Challenge\n",
    "1. It's like finding a needle in a haystack when searching through all these gems.\n",
    "2. Each book has a wealth of information, but finding specific topics can be time-consuming.\n",
    "\n",
    "\n",
    "It's time to craft a solution that transforms this exploration into an effortless search!\n",
    "\n",
    "\n",
    "### Seamless Exploration\n",
    "Embark on a seamless exploration of your chosen topic within a curated folder of PDFs. This AI aims to sift through the complexities, presenting you with a refined selection of documents that genuinely matter. \n",
    "\n",
    "### Extraction with Purpose\n",
    "Gone are the days of information overload. This system does not just search; it extracts meaningful information, distilling the essence of each document and focusing only on the most relevant and impactful details. \n",
    "\n",
    "This system is more than a tool; it's your key to unlocking the vast power of knowledge. Focusing on specific topics and information within curated PDF files brings you precisely the information you seek—focused intelligence is at your fingertips.\n",
    "\n",
    "\n",
    "### Technologies and Tools Used:\n",
    "\n",
    "- Programming Language: Python\n",
    "- PDF Parsing Libraries: PyPDF2, pdfplumber\n",
    "- Topic Modeling: Gensim - Latent Dirichlet Allocation (LDA)\n",
    "- Search Mechanism: TF-IDF\n",
    "- Named Entity Recognition: spaCy \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab3ef2d",
   "metadata": {},
   "source": [
    "## 1. Dataset Preparation: E-Book Collection\n",
    "\n",
    "- **Number of E-Books:** 891\n",
    "- **Total Size:** 14.77 GB\n",
    "- **Topic Focus:** Data Science and Machine Learning\n",
    "- **Compilation Date:** February 23, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59066c8",
   "metadata": {},
   "source": [
    "**Library Essentials:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6aa86a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "from tqdm import tqdm\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import spacy\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b7a46d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Number of E-books:   891 PDF files accessed\n",
      "2. Total Size: \t\t14087.48 MB\n",
      "\n",
      "\n",
      " PDF files found in the source folder:\n",
      "\n",
      "['Mastering Machine Learning with Python in Six Steps A Practical Implementation Guide to Predictive Data Analytics Using Python by Manohar Swamynathan (auth.).pdf', 'Football Hackers The Science and Art of a Data Revolution by Christoph Biermann.pdf', 'Hyperparameter Optimization in Machine Learning Make Your Machine Learning and Deep Learning Models More Efficient by Tanay Agrawal.pdf', 'The hundred-page machine learning book by Burkov, Andriy.pdf', 'Signal Processing and Machine Learning for Brain–Machine Interfaces by Toshihisa Tanaka, Mahnaz Arvaneh.pdf', 'Financial signal processing and machine learning by Akansu, Ali N. Kulkarni, Sanjeev Malioutov, Dmitry.pdf', 'XML and Web Technologies for Data Sciences with R by Deborah Nolan, Duncan Temple Lang (auth.).pdf', 'Fundamentals of Machine Learning for Predictive Data Analytics Algorithms, Worked Examples, and Case Studies by John D. Kelleher, Brian Mac Namee, Aoife DArcy.pdf', 'Data Science Algorithms in a Week Top 7 algorithms for computing, data analysis, and machine learning by David Natingga.pdf', 'Fundamentals of Deep Learning Designing Next-Generation Machine Intelligence Algorithms by Nikhil Buduma, Nicholas Locascio.pdf', 'Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow by Aurelien Geron.pdf', 'Machine Learning a Practical Approach on the Statistical Learning Theory by Antonelli Ponti, Moacir Fernandes de Melo, Dirce.pdf', 'Analytical Skills for AI and Data Science Building Skills for an AI-Driven Enterprise by Daniel Vaughan.pdf']\n"
     ]
    }
   ],
   "source": [
    "# Access PDF files in a designated folder \n",
    "def access_pdfs(source_folder, sample_size=13):\n",
    "    # View the files\n",
    "    pdf_files = [file for file in os.listdir(source_folder) if file.endswith(\".pdf\")]\n",
    "\n",
    "    # Check # and Total Size of the files\n",
    "    total_pdf_files = len(pdf_files)\n",
    "    total_size = sum(os.path.getsize(os.path.join(source_folder, pdf_file)) for pdf_file in pdf_files) / (1024 * 1024)\n",
    "\n",
    "    # Data Preview\n",
    "    print(f\"1. Number of E-books:   {total_pdf_files} PDF files accessed\")\n",
    "    print(f\"2. Total Size: \\t\\t{total_size:.2f} MB\")\n",
    "    print(f\"\\n\\n PDF files found in the source folder:\\n\\n{pdf_files[:sample_size]}\")  # Display only the specified sample size\n",
    "    accessed_pdf_files = [os.path.join(source_folder, pdf_file) for pdf_file in pdf_files]\n",
    "    return accessed_pdf_files[:sample_size]\n",
    "\n",
    "source_folder = \"/Users/mialaarnigarcia/Desktop/Project_DS_and_ML_Folder\"\n",
    "\n",
    "# Apply the function to the accessed e-books\n",
    "pdf_files_found = access_pdfs(source_folder, sample_size=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40273257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mastering Machine Learning with Python in Six ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Football Hackers The Science and Art of a Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hyperparameter Optimization in Machine Learnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The hundred-page machine learning book by Burk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Signal Processing and Machine Learning for Bra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Financial signal processing and machine learni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XML and Web Technologies for Data Sciences wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Fundamentals of Machine Learning for Predictiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Science Algorithms in a Week Top 7 algori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fundamentals of Deep Learning Designing Next-G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Hands-on Machine Learning with Scikit-Learn, K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Machine Learning a Practical Approach on the S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Analytical Skills for AI and Data Science Buil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            File Name\n",
       "0   Mastering Machine Learning with Python in Six ...\n",
       "1   Football Hackers The Science and Art of a Data...\n",
       "2   Hyperparameter Optimization in Machine Learnin...\n",
       "3   The hundred-page machine learning book by Burk...\n",
       "4   Signal Processing and Machine Learning for Bra...\n",
       "5   Financial signal processing and machine learni...\n",
       "6   XML and Web Technologies for Data Sciences wit...\n",
       "7   Fundamentals of Machine Learning for Predictiv...\n",
       "8   Data Science Algorithms in a Week Top 7 algori...\n",
       "9   Fundamentals of Deep Learning Designing Next-G...\n",
       "10  Hands-on Machine Learning with Scikit-Learn, K...\n",
       "11  Machine Learning a Practical Approach on the S...\n",
       "12  Analytical Skills for AI and Data Science Buil..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# See samples\n",
    "pdf_file_names = [os.path.basename(pdf_file) for pdf_file in pdf_files_found]\n",
    "df = pd.DataFrame({\"File Name\": pdf_file_names})\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29413fc2",
   "metadata": {},
   "source": [
    "Separate the Author Name from the PDF File Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96d48669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>E-Book Title</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mastering Machine Learning with Python in Six ...</td>\n",
       "      <td>Mastering Machine Learning with Python in Six ...</td>\n",
       "      <td>Manohar Swamynathan (auth.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Football Hackers The Science and Art of a Data...</td>\n",
       "      <td>Football Hackers The Science and Art of a Data...</td>\n",
       "      <td>Christoph Biermann</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hyperparameter Optimization in Machine Learnin...</td>\n",
       "      <td>Hyperparameter Optimization in Machine Learnin...</td>\n",
       "      <td>Tanay Agrawal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The hundred-page machine learning book by Burk...</td>\n",
       "      <td>The hundred-page machine learning book</td>\n",
       "      <td>Burkov, Andriy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Signal Processing and Machine Learning for Bra...</td>\n",
       "      <td>Signal Processing and Machine Learning for Bra...</td>\n",
       "      <td>Toshihisa Tanaka, Mahnaz Arvaneh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Financial signal processing and machine learni...</td>\n",
       "      <td>Financial signal processing and machine learning</td>\n",
       "      <td>Akansu, Ali N. Kulkarni, Sanjeev Malioutov, Dm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XML and Web Technologies for Data Sciences wit...</td>\n",
       "      <td>XML and Web Technologies for Data Sciences with R</td>\n",
       "      <td>Deborah Nolan, Duncan Temple Lang (auth.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Fundamentals of Machine Learning for Predictiv...</td>\n",
       "      <td>Fundamentals of Machine Learning for Predictiv...</td>\n",
       "      <td>John D. Kelleher, Brian Mac Namee, Aoife DArcy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Science Algorithms in a Week Top 7 algori...</td>\n",
       "      <td>Data Science Algorithms in a Week Top 7 algori...</td>\n",
       "      <td>David Natingga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fundamentals of Deep Learning Designing Next-G...</td>\n",
       "      <td>Fundamentals of Deep Learning Designing Next-G...</td>\n",
       "      <td>Nikhil Buduma, Nicholas Locascio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Hands-on Machine Learning with Scikit-Learn, K...</td>\n",
       "      <td>Hands-on Machine Learning with Scikit-Learn, K...</td>\n",
       "      <td>Aurelien Geron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Machine Learning a Practical Approach on the S...</td>\n",
       "      <td>Machine Learning a Practical Approach on the S...</td>\n",
       "      <td>Antonelli Ponti, Moacir Fernandes de Melo, Dirce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Analytical Skills for AI and Data Science Buil...</td>\n",
       "      <td>Analytical Skills for AI and Data Science Buil...</td>\n",
       "      <td>Daniel Vaughan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            File Name  \\\n",
       "0   Mastering Machine Learning with Python in Six ...   \n",
       "1   Football Hackers The Science and Art of a Data...   \n",
       "2   Hyperparameter Optimization in Machine Learnin...   \n",
       "3   The hundred-page machine learning book by Burk...   \n",
       "4   Signal Processing and Machine Learning for Bra...   \n",
       "5   Financial signal processing and machine learni...   \n",
       "6   XML and Web Technologies for Data Sciences wit...   \n",
       "7   Fundamentals of Machine Learning for Predictiv...   \n",
       "8   Data Science Algorithms in a Week Top 7 algori...   \n",
       "9   Fundamentals of Deep Learning Designing Next-G...   \n",
       "10  Hands-on Machine Learning with Scikit-Learn, K...   \n",
       "11  Machine Learning a Practical Approach on the S...   \n",
       "12  Analytical Skills for AI and Data Science Buil...   \n",
       "\n",
       "                                         E-Book Title  \\\n",
       "0   Mastering Machine Learning with Python in Six ...   \n",
       "1   Football Hackers The Science and Art of a Data...   \n",
       "2   Hyperparameter Optimization in Machine Learnin...   \n",
       "3              The hundred-page machine learning book   \n",
       "4   Signal Processing and Machine Learning for Bra...   \n",
       "5    Financial signal processing and machine learning   \n",
       "6   XML and Web Technologies for Data Sciences with R   \n",
       "7   Fundamentals of Machine Learning for Predictiv...   \n",
       "8   Data Science Algorithms in a Week Top 7 algori...   \n",
       "9   Fundamentals of Deep Learning Designing Next-G...   \n",
       "10  Hands-on Machine Learning with Scikit-Learn, K...   \n",
       "11  Machine Learning a Practical Approach on the S...   \n",
       "12  Analytical Skills for AI and Data Science Buil...   \n",
       "\n",
       "                                               Author  \n",
       "0                         Manohar Swamynathan (auth.)  \n",
       "1                                  Christoph Biermann  \n",
       "2                                       Tanay Agrawal  \n",
       "3                                      Burkov, Andriy  \n",
       "4                    Toshihisa Tanaka, Mahnaz Arvaneh  \n",
       "5   Akansu, Ali N. Kulkarni, Sanjeev Malioutov, Dm...  \n",
       "6           Deborah Nolan, Duncan Temple Lang (auth.)  \n",
       "7      John D. Kelleher, Brian Mac Namee, Aoife DArcy  \n",
       "8                                      David Natingga  \n",
       "9                    Nikhil Buduma, Nicholas Locascio  \n",
       "10                                     Aurelien Geron  \n",
       "11   Antonelli Ponti, Moacir Fernandes de Melo, Dirce  \n",
       "12                                     Daniel Vaughan  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract title and author for easy scan\n",
    "df['E-Book Title'] = df['File Name'].str.extract(r'^(.*?) by ')\n",
    "df['Author'] = df['File Name'].str.extract(r' by (.*)\\.pdf$')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30eccce",
   "metadata": {},
   "source": [
    "## 2. PDF Parsing:\n",
    "\n",
    "Use a PDF parsing library such as PyPDF2 or pdfplumber in Python to extract text content from the PDF documents.\n",
    "\n",
    "\n",
    "- We will go through each PDF file in the pdf_files_found list, open it using pdfplumber, iterate through its pages, and print the text content of each page along with the file name and page number.\n",
    "- To focus on extracting and manipulating content, PyPDF2, an open-source library, provides essential functionalities that makes it easy to work with PDF files. \n",
    "- Also a cross-platform compatible making it usable in different operating systems.\n",
    "\n",
    "Note: \n",
    "\n",
    "For complex or encrypted PDFs, there are more suitable libraries available aside from pdfplumber. There is also PyMuPDF or PpyPDFium may be suitable depending on the requirements of a project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82f10e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████████████████████| 13/13 [06:01<00:00, 27.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now extracted_text_list contains the texts from all PDFs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Storage of extracted texts from all PDFs\n",
    "extracted_text_list = []\n",
    "\n",
    "# Iterate through each PDF file with tqdm\n",
    "for pdf_file in tqdm(pdf_files_found, desc=\"Processing PDFs\"):\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        # Iterate through pages and extract text\n",
    "        for page_number in range(len(pdf.pages)):\n",
    "            page = pdf.pages[page_number]\n",
    "            text = page.extract_text()\n",
    "\n",
    "            # Append the extracted text to the list\n",
    "            extracted_text_list.append(text)\n",
    "\n",
    "print(\"Now extracted_text_list contains the texts from all PDFs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a4557a",
   "metadata": {},
   "source": [
    "## 3. Topic Modeling:\n",
    "- Leverage Gensim's Latent Dirichlet Allocation - LDA to identify key topics within the extracted text.\n",
    "\n",
    "\n",
    "- Each document will be represented as bag of words because Gensim's LDA model expects its input in the form of corpus.\n",
    "\n",
    "- All extracted text will be transformed into a single string, the entire corpus will be treated a \"single document\" for LDA modeling.\n",
    "\n",
    "- We tokenized the extracted text, created a dictionary and a corpus, and trained an LDA model.\n",
    "\n",
    "- The num_topics parameter specifies the number of topics I want the model to identify. Parameters may be adjusted based on our dataset and needs.\n",
    "\n",
    "- After training the base model, LDA, with the subset of our data to recognize the topics, the output will represent the \"identified topics\" along with the \"associated words\", and their \"weights\" within each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e19213a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.071*\"the\" + 0.028*\"of\" + 0.028*\"to\" + 0.026*\"a\" + 0.022*\"and\" + '\n",
      "  '0.019*\"in\" + 0.017*\"is\" + 0.012*\"for\" + 0.011*\"we\" + 0.011*\"that\"'),\n",
      " (1,\n",
      "  '0.047*\"the\" + 0.016*\"of\" + 0.015*\"to\" + 0.014*\"and\" + 0.014*\"a\" + '\n",
      "  '0.013*\"in\" + 0.010*\"is\" + 0.008*\"that\" + 0.007*\"we\" + 0.007*\"for\"'),\n",
      " (2,\n",
      "  '0.001*\"the\" + 0.001*\"of\" + 0.000*\"to\" + 0.000*\"and\" + 0.000*\"in\" + '\n",
      "  '0.000*\"a\" + 0.000*\"is\" + 0.000*\"we\" + 0.000*\"this\" + 0.000*\"for\"')]\n"
     ]
    }
   ],
   "source": [
    "# Combine all extracted text into a single string\n",
    "combined_text = ' '.join(extracted_text_list)\n",
    "\n",
    "# Preprocess the text (tokenization, removing stop words, etc.)\n",
    "# For simplicity, we'll use a basic tokenization here.\n",
    "tokenized_text = [word for word in combined_text.lower().split() if word.isalnum()]\n",
    "\n",
    "# Dictionary from the tokenized text and a corpus that represents the document as a bag of words\n",
    "dictionary = corpora.Dictionary([tokenized_text])\n",
    "corpus = [dictionary.doc2bow(tokenized_text)]\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = models.LdaModel(corpus, num_topics=3, id2word=dictionary, passes=15)\n",
    "\n",
    "# Print the topics\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bcb33e",
   "metadata": {},
   "source": [
    "Output Interpretation:\n",
    "\n",
    "The output represents the identified topics along with the associated words and their weights within each topic. Each tuple in the list corresponds to a topic, and the topics are represented as a combination of words with their associated weights.\n",
    "\n",
    "For example, in the first topic (index 0):\n",
    "\n",
    " - The word \"the\" has a weight of 0.073.\n",
    " - The word \"in\" has a weight of 0.043.\n",
    " - Similarly, other words and their weights are listed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11706b6f",
   "metadata": {},
   "source": [
    "## 4. Search Functionality:\n",
    "- We need a search mechanism that takes a user-inputted query and searches for relevant documents within the PDFs.\n",
    "- For efficient text searching, let us utilize techniques such as TF-IDF (Term Frequency-Inverse Document Frequency)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8c4e79",
   "metadata": {},
   "source": [
    "Why TF-IDF?\n",
    "- TF-IDF vectorizer from scikit-learn can convert text data into a numerical format. It then calculates the cosine similarity between the user query and each document, identifying the most relevant documents based on similarity scores.\n",
    "\n",
    "Why Cosine Similarity?\n",
    "- Cosine similarity is used for comparing similarity between two vectors. \n",
    "- In our case, the vector representations of the \"user query\" and \"each document\"\n",
    "- It measures the cosine angle between vectors in a multi-dimensional space.\n",
    "\n",
    "Cosine Similarity Merit:\n",
    "- It helps in comparing documents of different lengths and identify the most relevant documents by quantifying the similarity in the directions of their vector representations.\n",
    "- Usable when the magnitude of vectors is not important as the directional aspect.\n",
    "- A scale-invarient - meaning it focuses on the orientation of vectors rather than their length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a90edb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant documents:\n",
      "\n",
      "Document 3212: Decision Trees\n",
      "Without further knowledge, we would not be able to classify each row correctly.\n",
      "Fortunately, there is one more question that can be asked about each row which classifies\n",
      "each row correctly. For the row with the attribute water=cold, the swimming preference is\n",
      "no. For the row with the attribute water=warm, the swimming preference is yes.\n",
      "To summarize, starting with the root node, we ask a question at every node and based on\n",
      "the answer, we move down the tree until we reach a leaf node where we find the class of\n",
      "the data item corresponding to those answers.\n",
      "This is how we can use a ready-made decision tree to classify samples of the data. But it is\n",
      "also important to know how to construct a decision tree from the data.\n",
      "Which attribute has a question at which node? How does this reflect on the construction of\n",
      "a decision tree? If we change the order of the attributes, can the resulting decision tree\n",
      "classify better than another tree?\n",
      "Information theory\n",
      "Information theory studies the quantification of information, its storage and\n",
      "communication. We introduce concepts of information entropy and information gain that\n",
      "are used to construct a decision tree using ID3 algorithm.\n",
      "Information entropy\n",
      "Information entropy of the given data measures the least amount of the information\n",
      "necessary to represent a data item from the given data. The unit of the information entropy\n",
      "is a familiar unit - a bit and a byte, a kilobyte, and so on. The lower the information entropy,\n",
      "the more regular the data is, the more pattern occurs in the data and thus less amount of the\n",
      "information is necessary to represent it. That is why compression tools on the computer can\n",
      "take large text files and compress them to a much smaller size, as words and word\n",
      "expressions keep reoccurring, forming a pattern.\n",
      "[ 53 ]\n",
      "\n",
      "Document 2989: 640 Chapter11 BeyondPrediction: ReinforcementLearning\n",
      "abouttheworld,particularlyasepisodesmightcoverhundredsorthousandsoftime-steps.\n",
      "Instead, we collapse this information into a single representation, referred to as a state.\n",
      "The state at time-step t, s, should contain all the important information about the envi-\n",
      "t\n",
      "ronment at that time-step, any important information about what has been happening in\n",
      "theenvironmentatprecedingtime-steps,andanyimportantinformationabouttheinternal\n",
      "composition of the agent. For example, for a robot deployed within a hospital to deliver\n",
      "equipmenttooperatingtheaters,thestatemightincludetherobot’spositionintheenviron-\n",
      "ment,thepositionsofpeoplenearby,whethertherobotisonthewaytocollectitemsorto\n",
      "deliverthem,andthecurrentlevelsoftherobot’sbatteries.\n",
      "In Figure 11.1[639] we show how the observations made about the environment at time-\n",
      "steptareconvertedintoastate,s,usingastategenerationfunction,φ. Inmanycases,if\n",
      "t\n",
      "theenvironmentisfullyobservablethisfunctionisasimpleidentityfunctionbecausethe\n",
      "observationfullydefinesthestate. Itisalsopossible,however,forthisfunctiontobemore\n",
      "elaborate when the observations over multiple time-steps are accumulated into a state.1\n",
      "Usingstatesinsteadofobservations,Equation(11.1)[639]canberestated2\n",
      "ps ,a ,r q,ps ,a ,r q,ps ,a ,r q,...,ps ,a ,r q (11.2)\n",
      "1 1 1 2 2 2 3 3 3 e e e\n",
      "Weseeinsubsequentexamplesthatdesigninggoodstaterepresentationsisoneofthearts\n",
      "ofreinforcementlearning.\n",
      "The goal of the intelligent agent is to complete a task as successfully as possible. To\n",
      "framethereinforcementlearningproblem,thisneedstobemoreformallydefined—what\n",
      "doesitmeantosuccessfullycompleteatask? Thenextsectionexploresthis.\n",
      "11.2.2 FundamentalsofReinforcementLearning\n",
      "Thefundamentalideaunderpinningreinforcementlearningisthattheonlygoalofanin-\n",
      "telligent agent is to maximize cumulative reward across an episode.3 The cumulative\n",
      "rewardearnedacrossanepisodeisreferredtoasthereturnfromtheepisodeandcanbe\n",
      "definedas\n",
      "1.Environmentsinwhichthestatecontainsallinformationabouttheenvironmentandanyagentsinitareknown\n",
      "asfullyobservableenvironments.Environmentsinwhichthisisnotthecaseareknownaspartiallyobservable\n",
      "environments. Theuseofastategenerationfunctionallowsustotreatsomepartiallyobservableenvironments\n",
      "asiftheywerefullyobservableandapplythemechanicsofreinforcementlearningwhereotherwiseitwouldnot\n",
      "bepossible.\n",
      "2.Thereissomeargumentinthereinforcementlearningliteratureaboutwhethertherewardthatfollowsan\n",
      "action,at,takeninastate,st,shouldbereferredtoasrtorrt`1.Theargumentstemsfromadisagreementabout\n",
      "whenadiscretemomentoftimeends—aftertheactioncompletesoraftertherewardisreceived?Fromacompu-\n",
      "tationalpointofviewitmakesnodifference,aslongasconsistencyismaintainedinnotationandcomputation.\n",
      "Throughoutthischapterweuserttorefertotherewardreceivedaftertakinganaction,at,attime-stept.\n",
      "3.ThisiscapturedinSutton’srewardhypothesis(SuttonandBarto,2018):“Thatallofwhatwemeanbygoals\n",
      "andpurposescanbewellthoughtofasmaximizationoftheexpectedvalueofthecumulativesumofareceived\n",
      "scalarsignal(reward).”\n",
      "\n",
      "Document 3215: Decision Trees\n",
      "S ={(small,cold,no),(small,warm,no)}\n",
      "small\n",
      "S ={(good,cold,no),(good,warm,yes)}\n",
      "good\n",
      "The information entropy of S is E(S)=-(1/6)*log(1/6)-(5/6)*log(5/6)~0.65002242164.\n",
      "2 2\n",
      "The information entropy of the partitions is:\n",
      "E(S )=-(2/2)*log(2/2)=-log(1)=0 since all instances have the class no.\n",
      "none 2 2\n",
      "E(S )=0 for a similar reason.\n",
      "small\n",
      "E(S )=-(1/2)*log(1/2)=1\n",
      "good 2\n",
      "Therefore, the information gain is:\n",
      "IG(S,swimming suit)=E(S)-[(2/6)*E(S )+(2/6)*E(S )+(2/6)*E(S )]\n",
      "none small good\n",
      "=0.65002242164-(1/3)=0.3166890883\n",
      "If we chose the attribute water temperature to partition the set S, what would be the\n",
      "information gain IG(S,water temperature)? The water temperature partitions the set S into\n",
      "the following sets:\n",
      "S ={(none,cold,no),(small,cold,no),(good,cold,no)}\n",
      "cold\n",
      "S ={(none,warm,no),(small,warm,no),(good,warm,yes)}\n",
      "warm\n",
      "Their entropies are:\n",
      "E(S )=0 as all instances are classified as no.\n",
      "cold\n",
      "E(S )=-(2/3)*log(2/3)-(1/3)*log(1/3)~0.91829583405\n",
      "warm 2 2\n",
      "Therefore, the information gain from partitioning the set S by the attribute water\n",
      "temperature is:\n",
      "IG(S,water temperature)=E(S)-[(1/2)*E(S )+(1/2)*E(S )]\n",
      "cold warm\n",
      "= 0.65002242164-0.5*0.91829583405=0.19087450461\n",
      "This is less than IG(S,swimming suit). Therefore, we can gain more information about the set\n",
      "S (the classification of its instances) by partitioning it per the attribute swimming suit\n",
      "instead of the attribute water temperature. This finding will be the basis of the ID3\n",
      "algorithm constructing a decision tree in the next section.\n",
      "[ 56 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# User-inputted query\n",
    "user_query = \"important information\"\n",
    "\n",
    "# Preprocess the user query \n",
    "user_query_tokens = [word for word in user_query.lower().split() if word.isalnum()]\n",
    "\n",
    "# Preprocess the extracted text\n",
    "preprocessed_text_list = [\" \".join([word for word in text.lower().split() if word.isalnum()]) for text in extracted_text_list]\n",
    "\n",
    "# Combine the user query and preprocessed text\n",
    "all_texts = preprocessed_text_list + [\" \".join(user_query_tokens)]\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "\n",
    "# Calculate cosine similarity between the user query and each document\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1]).flatten()\n",
    "\n",
    "# Get the indices of documents with the highest similarity - Top 3\n",
    "top_indices = cosine_similarities.argsort()[:-4:-1] \n",
    "\n",
    "print(\"Most relevant documents:\\n\")\n",
    "for index in top_indices:\n",
    "    print(f\"Document {index + 1}: {extracted_text_list[index]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3570ef72",
   "metadata": {},
   "source": [
    "Findings:\n",
    "1. The top 3 topics with the highest similarity using cosine similarity: machine learning of course, decision trees, and reinforcement learning.\n",
    "2. There's a common theme of information theory, particularly in the context of decision trees and reinforcement learning.\n",
    "3. The documents provide a mix of theoretical concepts and practical examples, making them relevant for understanding and implementing machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97095e1a",
   "metadata": {},
   "source": [
    "## 5. Information Extraction:\n",
    "\n",
    "\n",
    "#### The Magic of spaCy\n",
    "\n",
    "Imagine diving into the vast sea of books, each page filled with the beauty of language. To make sense of this ocean of words, we needed a guide, and spaCy emerged as the perfect companion.\n",
    "\n",
    "#### Breaking it Down: The Art of Tokenization\n",
    "- In the realm of text, words are the building blocks. spaCy's magic lies in its ability to break down sentences into these meaningful elements – words, punctuation, and more. It's like deciphering the secret code of language.\n",
    "\n",
    "#### Understanding the Dance of Words: Part-of-Speech Tagging\n",
    "- Words, like actors on a stage, play different roles in a sentence. spaCy's Part-of-Speech tagging helps us identify these roles – who is the noun, the verb, or the adjective? It's like unraveling the intricate dance of language.\n",
    "\n",
    "#### Spotting the Stars: Named Entity Recognition (NER)\n",
    "- In our exploration, we encountered stars – named entities like people, places, and organizations. spaCy's NER capabilities shine in spotting these stars, helping us extract specific information and bring it into the spotlight.\n",
    "\n",
    "#### The Need for Speed: spaCy’s Efficiency\n",
    "- Navigating through the vast library of books is no small feat. SpaCy was designed with speed in mind, swiftly processing large volumes of text. It's like having a nimble guide leading you through the pages at a pace that keeps up with your curiosity.\n",
    "\n",
    "#### The Treasure Map: Pre-trained Models\n",
    "- Picture this – instead of starting from scratch, spaCy comes with pre-trained models, like a treasure map for multiple languages. It saves time and effort, guiding us to the hidden gems in the world of words.\n",
    "\n",
    "#### Harmony in Workflows: Integration with Machine Learning\n",
    "- Our journey doesn’t end with understanding words; we want to weave the insights into a larger tapestry. SpaCy seamlessly integrates with machine learning pipelines, allowing us to incorporate language understanding into our broader exploration.\n",
    "\n",
    "#### spaCy becomes the guide, the decoder, and the companion on our literary journey. It transforms the language of books into a tapestry of insights, making our exploration not just efficient but also easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9fa8346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c17eeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Keywords:\n",
      "\n",
      "['Contents', 'Polynomial', 'Regression', 'Multivariate', 'Regression', 'Multicollinearity', 'Variation', 'Inflation', 'Factor', 'VIF']\n"
     ]
    }
   ],
   "source": [
    "# Choose a document index of the document to analyze\n",
    "document_index = 7 \n",
    "\n",
    "# Get the text content of the chosen document\n",
    "document_text = extracted_text_list[document_index]\n",
    "\n",
    "# Use spaCy to preprocess\n",
    "doc = nlp(document_text)\n",
    "\n",
    "# Extract keywords\n",
    "keywords = [token.text for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "num_keywords_to_display = 10  \n",
    "top_keywords = keywords[:num_keywords_to_display]\n",
    "\n",
    "# Print extracted keywords\n",
    "print(f\"Top {num_keywords_to_display} Keywords:\\n\")\n",
    "print(top_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a321cb",
   "metadata": {},
   "source": [
    "## 6. Input queries and Extract Information\n",
    "- A simple user interface that allows users to input queries and view the extracted information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11223258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt the user to enter a topic or query\n",
    "user_query = input(\"Enter a topic or query: \")\n",
    "\n",
    "# Perform a search to find relevant documents\n",
    "relevant_documents = []\n",
    "\n",
    "for i, document_text in enumerate(extracted_text_list):\n",
    "    # Perform your search logic here, for example, checking if the user query is in the document text\n",
    "    if user_query.lower() in document_text.lower():\n",
    "        relevant_documents.append(i)\n",
    "\n",
    "# Display the information from relevant documents\n",
    "for document_index in relevant_documents:\n",
    "    print(f\"Extracted Information from Document {document_index + 1}:\")\n",
    "    print(extracted_text_list[document_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d302176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf93f917c2504b469afedbbe5016303f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Search:', placeholder='Enter a topic or query.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302e640c49c04e98978ae507f23bf555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Search', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839161a044454157b8a4dfea899c5e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def search_and_display(query):\n",
    "    \n",
    "\n",
    "    for i, document_text in enumerate(extracted_text_list):\n",
    "        if query.lower() in document_text.lower():\n",
    "            relevant_documents.append(i)\n",
    "\n",
    "    output.clear_output()\n",
    "\n",
    "    with output:\n",
    "        # Display additional information\n",
    "        total_documents = len(extracted_text_list)\n",
    "        print(f\"1. Number of E-books related to the topic: {len(relevant_documents)}\")\n",
    "        print(f\"\\n2. List of Documents: \\n{', '.join([f'Document {i+1}' for i in relevant_documents])}\")\n",
    "        print(\"3. Information extracted:\")\n",
    "\n",
    "        # Display the information from relevant documents\n",
    "        for document_index in relevant_documents:\n",
    "            print(f\"Extracted Information from Document {document_index + 1}:\")\n",
    "            print(extracted_text_list[document_index])\n",
    "\n",
    "# Text input\n",
    "search_input = widgets.Text(placeholder=\"Enter a topic or query.\", description=\"Search:\")\n",
    "\n",
    "# Button \n",
    "search_button = widgets.Button(description=\"Search\")\n",
    "\n",
    "# Widget to display output\n",
    "output = widgets.Output()\n",
    "\n",
    "# Function to handle button click\n",
    "def on_search_button_click(b):\n",
    "    search_and_display(search_input.value)\n",
    "\n",
    "# Attach the function to the button click event\n",
    "search_button.on_click(on_search_button_click)\n",
    "\n",
    "# Display the widgets\n",
    "display(search_input, search_button, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181ceee2",
   "metadata": {},
   "source": [
    "## 7. Evaluation and Improvement:\n",
    "We can evaluate the accuracy and effectiveness of the system by comparing its search results and information extraction against a manually annotated set of relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e223ffa",
   "metadata": {},
   "source": [
    "## 8. Summary:\n",
    "Imagine you have 891 e-books or more, each holding the secrets of Data Science and Machine Learning.\n",
    "As a passionate book lover and knowledge seeker, navigating this treasure trove is a bit overwhelming.\n",
    "\n",
    "Going back to the Challenge:\n",
    "1. It's like finding a needle in a haystack when searching through all these gems.\n",
    "2. Each book has a wealth of information, but finding specific topics can be time-consuming.\n",
    "\n",
    "Introducing the Solution:\n",
    "- Enter Gensim's Latent Dirichlet Allocation (LDA) model, a tool for extracting hidden topics from a collection of documents.\n",
    "- But here's the twist: before unleashing the power of LDA, we combine all these e-books into a single string.\n",
    "\n",
    "Why the Merge?\n",
    "- LDA works with a \"corpus,\" treating each piece of text as a document.\n",
    "- Our definition of a \"document\" isn't each e-book but a continuous text block. So, we merge them into a single string.\n",
    "- This helps LDA capture relationships and patterns that might span across different parts of the documents.\n",
    "\n",
    "Path taken:\n",
    "- The combined text undergoes tokenization and preprocessing, transforming it into a clean, structured format.\n",
    "- Think of it as preparing the ingredients before cooking - ensuring everything is in order.\n",
    "\n",
    "Simplifying the Plot:\n",
    "- Merging simplifies the initial implementation. It's a quick way to test the LDA model on the entire corpus without worrying about document boundaries.\n",
    "- A pragmatic approach for experimentation, but not a one-size-fits-all solution.\n",
    "\n",
    "Keep in Mind:\n",
    "- This strategy might only suit some scenarios. If preserving the uniqueness of each document is crucial or if your documents are lengthy and contain different fields, this method might lose meaningful distinctions.\n",
    "- Combining or separating documents depends on what you want to achieve.\n",
    "\n",
    "\n",
    "**With the stage set, topics discovered, and information extracted, the search through the e-books becomes not just manageable but easier.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeec475",
   "metadata": {},
   "source": [
    "Sources:\n",
    "- Machine Learning Design Interview - Khang Pham\n",
    "- Analytical Skills for AI and Data Science- Daniel Vaughan\n",
    "- OpenAI. (2024). ChatGPT [Large language model]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30d8007",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
